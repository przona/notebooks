{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from colorama import Fore, Back, Style\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(url, debug=True):\n",
    "    time.sleep(1)\n",
    "    web_page = requests.get(url)\n",
    "    if web_page.status_code == 200:\n",
    "        if debug:\n",
    "            print(f\"retrieved web page {url} (200/{len(web_page.content)})\")\n",
    "    else:\n",
    "        print(Fore.RED, f\"web page {url} returned status code {web_page.status_code}\", Style.RESET_ALL)\n",
    "    return(web_page.content)\n",
    "\n",
    "\n",
    "\n",
    "def get_page_links(web_page, patterns=[]):\n",
    "    page_links = []\n",
    "    for a in BeautifulSoup(web_page, \"html.parser\").select('a'):\n",
    "        try:\n",
    "            href = a.get(\"href\")\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, href):\n",
    "                    page_links.append(href)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return(page_links)\n",
    "\n",
    "\n",
    "def get_web_pages(url, patterns=[], processed_urls=[], debug=True):\n",
    "    web_page = get_web_page(url, debug)\n",
    "    target_urls = get_page_links(web_page, patterns)\n",
    "    base_url = \"/\".join(url.split(\"/\")[0:3])\n",
    "    web_pages = {}\n",
    "    retrieved_urls = []\n",
    "    while len(set(target_urls)) > len(web_pages):\n",
    "        target_url = list(set(target_urls).difference(set(web_pages.keys())))[0]\n",
    "        if target_url in processed_urls:\n",
    "            web_pages[target_url] = \"PROCESSED\"\n",
    "            if debug:\n",
    "                print(f\"already processed {target_url}\")\n",
    "        elif not re.search(\"\\.html*$\",target_url) and not re.search(\"/[^.]*$\",target_url):\n",
    "            web_pages[target_url] = \"SKIPPED\"\n",
    "            if debug:\n",
    "                print(f\"skipped {target_url}\")\n",
    "        elif re.search(\"/gerelateerde_documenten/\", target_url) and \\\n",
    "             \"/\".join(target_url.split(\"/\")[6:]) in retrieved_urls:\n",
    "            web_pages[target_url] = \"DUPLICATE\"\n",
    "            if debug:\n",
    "                print(f\"duplicate {target_url}\")\n",
    "        else:\n",
    "            web_pages[target_url] = get_web_page(base_url+target_url, debug)\n",
    "            target_urls.extend(get_page_links(web_pages[target_url], patterns))\n",
    "            if re.search(\"/gerelateerde_documenten/\", target_url):\n",
    "                retrieved_urls.append(\"/\".join(target_url.split(\"/\")[6:]))\n",
    "    return(web_pages)\n",
    "\n",
    "\n",
    "def get_recommendation_list(web_pages):\n",
    "    recommendation_list = []\n",
    "    for key in web_pages:\n",
    "        for a in BeautifulSoup(web_pages[key], \"html.parser\").select('a'):\n",
    "            try:\n",
    "                href = a.get(\"href\")\n",
    "                if re.search(\"^/richtlijn/\", href):\n",
    "                    recommendation = href.split(\"/\")[2]\n",
    "                    if recommendation not in recommendation_list:\n",
    "                        recommendation_list.append(recommendation)\n",
    "            except TypeError:\n",
    "                pass\n",
    "    return(recommendation_list)\n",
    "\n",
    "\n",
    "def save_dict(dictionary, out_file_name, mode=\"w\"):\n",
    "    pd.DataFrame(dictionary, index=[0]).T.to_csv(out_file_name, header=False, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "\n",
    "main_web_pages = get_web_pages(BASE_URL, patterns=[\"^/\\?page=\\d+$\"])\n",
    "save_dict(main_web_pages, \"main_web_pages.csv\")\n",
    "print(f\"number of pages: {len(main_web_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_list = get_recommendation_list(main_web_pages)\n",
    "print(f\"found {len(recommendation_list)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTFILE = \"recommendation_web_pages.csv\"\n",
    "\n",
    "processed_urls = list(pd.read_csv(OUTFILE, header=None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "last_skipped = \"\"\n",
    "for recommendation in recommendation_list:\n",
    "    counter += 1\n",
    "    print(counter,recommendation)\n",
    "    if \"/richtlijn/\"+recommendation not in processed_urls:\n",
    "        recommendation_web_pages = get_web_pages(BASE_URL+\"/richtlijn/\"+recommendation,\n",
    "                                                 patterns=[\"^/richtlijn/\", \"^/gerelateerde_documenten\"],\n",
    "                                                 processed_urls = processed_urls,\n",
    "                                                 debug=False)\n",
    "        save_dict(recommendation_web_pages, OUTFILE, mode=\"a\")\n",
    "        processed_urls += list(recommendation_web_pages.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(main_web_pages['/?page=1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "for option in soup.select(\"option\"):\n",
    "    key = option.get(\"value\")\n",
    "    value = option.text\n",
    "    categories[key] = value\n",
    "del(categories[\"\"])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(categories.keys(), key=lambda key:int(key)):\n",
    "    print(key, categories[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "BASE_QUERY = \"/?query=&page=1&specialism=\"\n",
    "\n",
    "recommendations_per_category = {}\n",
    "for key in categories:\n",
    "    web_pages = get_web_pages(BASE_URL+BASE_QUERY+str(key),\n",
    "                              patterns=[\"^/\\?query=\\&page=\\d+\"],\n",
    "                              processed_urls=[BASE_URL+BASE_QUERY+str(key)])\n",
    "    web_pages[BASE_QUERY+str(key)] = get_web_page(BASE_URL+BASE_QUERY+str(key))\n",
    "    print(f\"category {key}; number of pages: {len(web_pages)}\")\n",
    "    recommendation_list = get_recommendation_list(web_pages)\n",
    "    print(f\"found {len(recommendation_list)} recommendations for category {key} {categories[key]}\\n\")\n",
    "    recommendations_per_category[key] = recommendation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "categories_per_recommendation = {}\n",
    "for category in recommendations_per_category:\n",
    "    for recommendation in recommendations_per_category[category]:\n",
    "        if recommendation not in categories_per_recommendation:\n",
    "            categories_per_recommendation[recommendation] = {}\n",
    "            for category in recommendations_per_category:\n",
    "                categories_per_recommendation[recommendation][category] = \" \"\n",
    "        categories_per_recommendation[recommendation][category] = \"+\"\n",
    "categories_per_recommendation = {r:categories_per_recommendation[r] for r in sorted(categories_per_recommendation.keys(),\\\n",
    "    key=lambda r:len([c for c in categories_per_recommendation[r] if categories_per_recommendation[r][c] == \"+\"]),reverse=True)} \n",
    "r_per_c = {c:{r:categories_per_recommendation[r][c] for r in categories_per_recommendation} for c in categories_per_recommendation[list(categories_per_recommendation.keys())[0]]}\n",
    "r_per_c = {c:r_per_c[c] for c in sorted(r_per_c.keys(), key=lambda c:len([r for r in r_per_c[c] if r_per_c[c][r] == \"+\"]), reverse=True)} \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(r_per_c).to_csv(\"richtlijnen-categorie.csv\", index_label=\"richtlijn\")\n",
    "pd.DataFrame(r_per_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
