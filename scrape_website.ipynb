{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from colorama import Fore, Back, Style\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "from IPython.display import clear_output\n",
    "from przona import *\n",
    "\n",
    "dummy = csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get web pages with python's request (2021-04-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_url(url):\n",
    "    if re.search(\"^https?://\", url, flags=re.IGNORECASE):\n",
    "        return(\"/\".join(url.split(\"/\")[:3]), \"/\"+\"/\".join(url.split(\"/\")[3:]))\n",
    "    else:\n",
    "        return(\"\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(url, debug=True):\n",
    "    assert re.search(r\"^https://\", url), f\"get_web_page: url has unexpected format: {url}\"\n",
    "    time.sleep(1)\n",
    "    web_page = requests.get(url)\n",
    "    if web_page.status_code == 200:\n",
    "        if debug:\n",
    "            print(f\"retrieved web page {url} (200/{len(web_page.content)})\")\n",
    "    else:\n",
    "        print(Fore.RED, f\"web page {url} returned status code {web_page.status_code}\", Style.RESET_ALL)\n",
    "    return(web_page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(web_page, patterns=[]):\n",
    "    page_links = []\n",
    "    for a in BeautifulSoup(web_page, \"html.parser\").select('a'):\n",
    "        try:\n",
    "            href = a.get(\"href\")\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, href):\n",
    "                    page_links.append(href)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_content(remote_file, url, content):\n",
    "    out_file_name = make_local_file_name(remote_file, url)\n",
    "    out_file_name_parts = out_file_name.split(\"/\")\n",
    "    for i in range(4, len(out_file_name_parts)):\n",
    "        if not os.path.isdir(\"/\".join(out_file_name_parts[0: i])):\n",
    "            os.mkdir(\"/\".join(out_file_name_parts[0: i]))\n",
    "    out_file = open(out_file_name, \"w\")\n",
    "    print(content.decode(\"utf8\"), file=out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_local_file_name(remote_file, url):\n",
    "    base_dir = f\"../data/{'.'.join(url.split('.')[-2:])}\"\n",
    "    if not os.path.isdir(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    remote_file_parts = remote_file.split(\"/\")\n",
    "    if re.search(r'\\.html$', remote_file):\n",
    "        dir_name = \"/\".join(remote_file_parts[:-1])\n",
    "        file_name = remote_file_parts[-1]\n",
    "    else:\n",
    "        dir_name = remote_file\n",
    "        file_name = \"index.html\"\n",
    "        remote_file = os.path.join(dir_name, file_name)\n",
    "    remote_file = re.sub(r\"^/\", \"\", remote_file)\n",
    "    out_file_name = os.path.join(base_dir, remote_file)\n",
    "    return(out_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_pages(url, patterns=[], processed_urls=[], debug=True):\n",
    "    base_url, remote_file = split_url(url)\n",
    "    if remote_file in processed_urls or re.search(r'\\.pdf$', remote_file) or re.search(r'\\?', remote_file):\n",
    "        return\n",
    "    out_file_name = make_local_file_name(remote_file, url)\n",
    "    if os.path.isfile(out_file_name) and not re.search(\"index.html$\", out_file_name):\n",
    "        processed_urls[remote_file] = \"\"\n",
    "        return\n",
    "    processed_urls[remote_file] = get_web_page(url, debug)\n",
    "    store_content(remote_file, url, processed_urls[remote_file])\n",
    "    page_links = get_page_links(processed_urls[remote_file], patterns)\n",
    "    for page_link in sorted(page_links):\n",
    "        get_web_pages(base_url + page_link, patterns, processed_urls, debug)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved web page https://www.ggzstandaarden.nl/ (200/2743)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url = \"https://richtlijnendatabase.nl/\"\n",
    "#patterns = [\"^/richtlijn/\", \"^/en/richtlijn/\"]\n",
    "url = \"https://www.ggzstandaarden.nl/\"\n",
    "patterns = [\"^/generieke-modules/\", \"^/richtlijnen/\", \"^/zorgstandaarden/\",]\n",
    "\n",
    "processed_urls = {}\n",
    "debug = True\n",
    "get_web_pages(url, patterns, processed_urls, debug)\n",
    "len(processed_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read html pages from disk (after wget download) 2021-03-31\n",
    "\n",
    "**WARNING**: at Linux, wget trunctuates long file names which results in information loss. There seems to be no solution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/richtlijnendatabase.nl/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index_file(data_dir, index_file_name=\"index.html\"):\n",
    "    infile = open(data_dir + index_file_name, \"r\")\n",
    "    index_file_data = \"\"\n",
    "    for line in infile:\n",
    "        index_file_data += line\n",
    "    infile.close()\n",
    "    return(index_file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(index_file_data):\n",
    "    recommendations = {}\n",
    "    soup = BeautifulSoup(index_file_data)\n",
    "    for a in soup.select(\"a\"):\n",
    "        href = a[\"href\"]\n",
    "        if re.search(\"^/richtlijn/\", href):\n",
    "            recommendation = \"/\".join(href.split(\"/\")[:3])\n",
    "            if not recommendation in recommendations:\n",
    "                recommendations[recommendation] = True\n",
    "        elif re.search(\"^/en/richtlijn/\", href):\n",
    "            recommendation = \"/\".join(href.split(\"/\")[:4])\n",
    "            if not recommendation in recommendations:\n",
    "                recommendations[recommendation] = True\n",
    "    return(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file_data = read_index_file(DATA_DIR)\n",
    "recommendations = get_recommendations(index_file_data)\n",
    "print(f\"found: {len(recommendations)} recommendations \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check /richtlijnen folder\n",
    "\n",
    "**WARNING**: the existance of a folder does not mean that the recommendation exists on the website. It might be that a folder contains a single file index.html, complaining about the recommendation not being present at the website (message *Oeps, pagina niet gevonden*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_files = sorted(os.listdir(DATA_DIR + \"richtlijn\"))\n",
    "recommendation_files = [\"/richtlijn/\" + file_name for file_name in recommendation_files]\n",
    "recommendation_files_en = sorted(os.listdir(DATA_DIR + \"en/richtlijn\"))\n",
    "recommendation_files_en = [\"/en/richtlijn/\" + file_name for file_name in recommendation_files_en]\n",
    "recommendation_files.extend(recommendation_files_en)\n",
    "print(f\"found: {len(recommendation_files)} recommendations \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(root, dirs, files):\n",
    "    file_texts = {}\n",
    "    for file_name in files:\n",
    "        file_name_with_directory = os.path.join(root, file_name)\n",
    "        if os.path.isfile(file_name_with_directory):\n",
    "            infile = open(file_name_with_directory, \"r\")\n",
    "            text = \"\"\n",
    "            for line in infile:\n",
    "                text += line\n",
    "            infile.close()\n",
    "            file_name_with_directory = \"/\" + \"/\".join(file_name_with_directory.split(\"/\")[3:])\n",
    "            file_texts[file_name_with_directory] = text\n",
    "    return file_texts\n",
    "\n",
    "def make_file_texts(data_dir):\n",
    "    file_texts = {}\n",
    "    for root, dirs, files in os.walk(data_dir + \"richtlijn\"):\n",
    "        for file_name in files:\n",
    "            file_texts.update(process(root, dirs, files))\n",
    "    for root, dirs, files in os.walk(data_dir + \"en/richtlijn\"):\n",
    "        for file_name in files:\n",
    "            file_texts.update(process(root, dirs, files))\n",
    "    return file_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    file_texts = make_file_texts(DATA_DIR)\n",
    "    len(file_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    save_dict(file_texts, \"csv/recommendation_web_pages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file_name in recommendation_files:\n",
    "    if file_name in recommendation_files_old:\n",
    "        status = {\"file_name\": file_name, \"status\": \"old\"}\n",
    "    else:\n",
    "        status = {\"file_name\": file_name, \"status\": \"new\"}\n",
    "    file_name_parts = file_name.split(\"/\")[1:]\n",
    "    status.update({i:file_name_parts[i] for i in range(0, len(file_name_parts))})\n",
    "    file_names.append(status)\n",
    "    \n",
    "for file_name in recommendation_files_old:\n",
    "    if file_name not in recommendation_files:\n",
    "        status = {\"file_name\": file_name, \"status\": \"removed\"}\n",
    "        file_name_parts = file_name.split(\"/\")[1:]\n",
    "        status.update({i:file_name_parts[i] for i in range(0, len(file_name_parts))})\n",
    "        file_names.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_names).sort_values(by=['file_name'])\n",
    "del df['file_name']\n",
    "df.to_csv(\"csv/recommendations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for file_name in file_texts.keys():\n",
    "    if file_name in file_texts_old:\n",
    "        status = {\"file_name\": file_name, \"status\": \"old\"}\n",
    "    else:\n",
    "        status = {\"file_name\": file_name, \"status\": \"new\"}\n",
    "    file_name_parts = file_name.split(\"/\")[1:]\n",
    "    status.update({i:file_name_parts[i] for i in range(0, len(file_name_parts))})\n",
    "    if not re.search(r'index.html$', file_name) or status[\"status\"] != \"new\":\n",
    "        file_names.append(status)\n",
    "\n",
    "for file_name in file_texts_old.keys():\n",
    "    if file_name not in file_texts:\n",
    "        status = {\"file_name\": file_name, \"status\": \"removed\"}\n",
    "        file_name_parts = file_name.split(\"/\")[1:]\n",
    "        status.update({i:file_name_parts[i] for i in range(0, len(file_name_parts))})\n",
    "        file_names.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_names).sort_values(by=['file_name'])\n",
    "del df['file_name']\n",
    "df.to_csv(\"csv/files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_OLD = \"../data/richtlijnendatabase.nl-20210315/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(DATA_DIR_OLD)\n",
    "recommendations_old = {}\n",
    "for file_name in file_names:\n",
    "    if re.search(\"index.html\", file_name):\n",
    "        index_file_data = read_index_file(DATA_DIR_OLD, index_file_name=file_name)\n",
    "        recommendations_new = get_recommendations(index_file_data)\n",
    "        recommendations_old.update(recommendations_new)\n",
    "print(len(recommendations_old.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dicts(dict1, dict2):\n",
    "    diff = []\n",
    "    for key in dict1:\n",
    "        if key not in dict2:\n",
    "            diff.append(key)\n",
    "    return(sorted(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disappeared = compare_dicts(recommendations_old, recommendations)\n",
    "(len(disappeared), disappeared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = compare_dicts(recommendations, recommendations_old)\n",
    "(len(new), new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_files_old = sorted(os.listdir(DATA_DIR_OLD + \"richtlijn\"))\n",
    "recommendation_files_old = [\"/richtlijn/\" + file_name for file_name in recommendation_files_old]\n",
    "recommendation_files_old_en = sorted(os.listdir(DATA_DIR_OLD + \"en/richtlijn\"))\n",
    "recommendation_files_old_en = [\"/en/richtlijn/\" + file_name for file_name in recommendation_files_old_en]\n",
    "recommendation_files_old.extend(recommendation_files_old_en)\n",
    "len(recommendation_files_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_files = sorted(os.listdir(DATA_DIR + \"richtlijn\"))\n",
    "recommendation_files = [\"/richtlijn/\" + file_name for file_name in recommendation_files]\n",
    "recommendation_files_en = sorted(os.listdir(DATA_DIR + \"en/richtlijn\"))\n",
    "recommendation_files_en = [\"/en/richtlijn/\" + file_name for file_name in recommendation_files_en]\n",
    "recommendation_files.extend(recommendation_files_en)\n",
    "len(recommendation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disappeared = compare_dicts(recommendation_files_old, recommendation_files)\n",
    "(len(disappeared), disappeared)\n",
    "#processed_urls = {}\n",
    "#for file_name in disappeared:\n",
    "#    get_web_pages(os.path.join(url, re.sub(r'^/', '', file_name)), [r'^' + file_name], processed_urls, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = compare_dicts(recommendation_files, recommendation_files_old)\n",
    "(len(new), new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts_old = make_file_texts(DATA_DIR_OLD)\n",
    "len(file_texts_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = {}\n",
    "for file_name in file_texts:\n",
    "    if not file_name in file_texts_old:\n",
    "        length = len(file_name)\n",
    "        if not length in lengths:\n",
    "            lengths[length] = 0\n",
    "        lengths[length] += 1\n",
    "        print(file_name)\n",
    "[(length, lengths[length]) for length in sorted(lengths.keys(), reverse=True)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download html pages (old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "CSV_DIR = \"csv/\"\n",
    "MAIN_WEB_PAGES_FILE = \"main_web_pages.csv\"\n",
    "PAGE1 = \"/?page=1\"\n",
    "RECOMMENDATIONS_FILE = \"recommendation_web_pages.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_web_pages = get_web_pages(BASE_URL+PAGE1, patterns=[\"^/\\?page=\\d+$\"])\n",
    "save_dict(main_web_pages, CSV_DIR+MAIN_WEB_PAGES_FILE)\n",
    "print(f\"number of pages: {len(main_web_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main_web_pages\n",
    "except NameError:\n",
    "    print(\"Reading main_web_pages from disk...\")\n",
    "    main_web_pages = read_dict(CSV_DIR+MAIN_WEB_PAGES_FILE)\n",
    "recommendation_list = get_recommendation_list(main_web_pages)\n",
    "print(f\"found {len(recommendation_list)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = read_dict(CSV_DIR+RECOMMENDATIONS_FILE, spy=True)\n",
    "processed_urls = list(web_pages.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_recommendations(recommendation_list, processed_urls, CSV_DIR+RECOMMENDATIONS_FILE, BASE_URL=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = read_dict(CSV_DIR+RECOMMENDATIONS_FILE, spy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive categories of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_QUERY = \"/?query=&page=1&specialism=\"\n",
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "CATEGORIES_FILE = \"categories.csv\"\n",
    "CSV_DIR = \"csv/\"\n",
    "MAIN_WEB_PAGES_FILE = \"main_web_pages.csv\"\n",
    "RECOMMENDATIONS_PER_CATEGORY_FILE = \"recommendations_per_category.csv\"\n",
    "PAGE1 = \"/?page=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(CSV_DIR+CATEGORIES_FILE):\n",
    "    categories = read_dict(CSV_DIR+CATEGORIES_FILE)\n",
    "else:\n",
    "    main_web_pages = read_dict(CSV_DIR+MAIN_WEB_PAGES_FILE)\n",
    "    categories = get_categories(main_web_pages[PAGE1])\n",
    "    save_dict(categories, CSV_DIR+CATEGORIES_FILE)\n",
    "if os.path.isfile(CSV_DIR+RECOMMENDATIONS_PER_CATEGORY_FILE):\n",
    "    recommendations_per_category = read_dict(CSV_DIR+RECOMMENDATIONS_PER_CATEGORY_FILE)\n",
    "else:\n",
    "    recommendations_per_category = get_recommendations_per_category(categories)\n",
    "    save_dict(recommendations_per_category, CSV_DIR+RECOMMENDATIONS_PER_CATEGORY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.min_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pretty_df = pretty_print(recommendations_per_category, CSV_DIR+\"richtlijnen-categorie.csv\")\n",
    "recommendations = get_recommendations(web_pages)\n",
    "for recommendation in recommendations:\n",
    "    if recommendation not in pretty_df.index:\n",
    "        pretty_df = pretty_df.append(pd.Series({i:' ' for i in pretty_df.iloc[0].index},name=recommendation))\n",
    "pretty_df.index = [x[:30] for x in pretty_df.index]\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DIR = \"csv/\"\n",
    "MAIN_WEB_PAGES_FILE = \"main_web_pages.csv\"\n",
    "RECOMMENDATIONS_FILE = \"recommendation_web_pages.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_web_pages = read_dict(CSV_DIR+MAIN_WEB_PAGES_FILE)\n",
    "web_pages = read_dict(CSV_DIR+RECOMMENDATIONS_FILE, spy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of main web pages: {len(main_web_pages)}\")\n",
    "print(f\"number of categories: {len(recommendations_per_category)}\")\n",
    "print(f\"number of recommendations: {len(get_recommendations(web_pages))}\")\n",
    "print(f\"number of web pages: {len(web_pages)}\")\n",
    "print(f\"file suffixes: {count_suffixes(web_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = \"/richtlijn/abdominoplastiek\"\n",
    "url_part = \"Algemene\"\n",
    "\n",
    "for href in get_links(web_pages, recommendation, url_part):\n",
    "    print(f\"url: {href}; web page size: {len(web_pages[href][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary in Dutch\n",
    "\n",
    "We hebben de webpagina's opgehaald van de website richtlijnendatabase.nl met als startpagina:\n",
    "\n",
    "https://richtlijnendatabase.nl/?page=1\n",
    "\n",
    "| Soort             | Aantal | Voorbeeld | Opmerking |\n",
    "| :---------------- | -----: | :-------- | :-------- |\n",
    "| hoofdpagina       |     42 | /?page=1 | 1-42 |\n",
    "| alle webpagina's  | 47.569 | /?page=1 | |\n",
    "| zonder extensie:  |    417 | /richtlijn/acne | |\n",
    "| extensie: .html   | 42.190 | /gerelateerde_documenten/bijlage/001094/1/90/Afkortingen.html | |\n",
    "| extensie: .pdf    |  4.435 | /gerelateerde_documenten/f/21504/Kennisdocument%20-%20Statines.pdf | |\n",
    "| extensie: .php    |    153 | /richtlijn/item/pagina.php?id=24679&richtlijn_id=480&tab=1 | |\n",
    "| extensie: .docx   |    135 | /gerelateerde_documenten/f/11337/Vragenlijst.docx | |\n",
    "| extensie: .pptx   |    129 | /gerelateerde_documenten/f/19293/presentatie%20richtlijn%20DCD.pptx | |\n",
    "| overige extensies |    110 | /gerelateerde_documenten/f/3691/Interventiegrenzen.doc | .doc:35 .htm:1 .jpg:16 .PNG: 1 .png:23 .ppt:14 .tif:6 .xls:3 .xlsx:11 |\n",
    "| richtlijnen       |    418 | /richtlijn/acne | |\n",
    "| categorieën       |     51 | /?query=&page=1&specialism=61 | 2-64 (missen: 10 20 23 24 26 32 35 38 39 41 42 48) |\n",
    "\n",
    "Onder de richtlijnen bleken identieke bijlagen gelinkt te zijn onder verschilende namen. Bijvoorbeeld, op de veertien pagina's voor \"/richtlijn/abdominoplastiek\" werd de bijlage Algemene inleiding.html viertien keer gelinkt vanuit dertien verschillende folders:\n",
    "\n",
    "* /gerelateerde_documenten/bijlage/015124/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/018015/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015130/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/018017/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/018010/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015128/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015131/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015129/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015125/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015126/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015127/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/018016/1/60/Algemene%20inleiding.html\n",
    "* /gerelateerde_documenten/bijlage/015132/1/60/Algemene%20inleiding.html\n",
    "\n",
    "Uit een analyse kwam naar voren dat het hier om twee verschillende documenten ging die konden worden onderscheiden op basis van de eerste drie cijfers in het eerste getal in het webadres (015 vs 018). Omdat we niet aan het webadres konden zien welke documenten uniek waren en welke duplicaten, hebben we alle webdocumenten opgehaald.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wget download check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "CSV_DIR = \"csv/\"\n",
    "RECOMMENDATIONS_FILE = \"recommendation_web_pages.csv\"\n",
    "WGET = \"../data/wget.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = read_dict(CSV_DIR+RECOMMENDATIONS_FILE, spy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../data/richtlijnendatabase.nl\"\n",
    "\n",
    "def find_missing_files():\n",
    "    file_counter = 0\n",
    "    missing_recommendations = []\n",
    "    urls = []\n",
    "    for original_url in web_pages:\n",
    "        url = urllib.parse.unquote(original_url)\n",
    "        url = re.sub(r'\\t', '%09', url)\n",
    "        if not os.path.isfile(BASE_DIR+url) and not os.path.isdir(BASE_DIR+url) and not os.path.isfile((BASE_DIR+url)[:244]):\n",
    "            if re.search(\"^/richtlijn/[^.]*$\", url): \n",
    "                missing_recommendations.append(url)\n",
    "            original_url = re.sub(\" \", \"%20\", original_url)\n",
    "            original_url = re.sub(\"ë\", \"%C3%AB\", original_url)\n",
    "            urls.append(original_url)\n",
    "    return(urls, missing_recommendations)\n",
    "\n",
    "urls, missing_recommendations = find_missing_files()\n",
    "print(f\"missing urls: {len(urls)}, of which missing recommendations: {len(missing_recommendations)}: {missing_recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGFILE = \"../data/logfile\"\n",
    "\n",
    "def find_cause_of_missing_files(urls):\n",
    "    status = 0\n",
    "    msgs = {}\n",
    "    url = \"\"\n",
    "    logfile = open(LOGFILE, \"r\")\n",
    "    for line in logfile:\n",
    "        if re.search(BASE_URL, line):\n",
    "            url = re.sub(\".*\"+BASE_URL, \"\", line.strip())\n",
    "            if url in urls:\n",
    "                msgs[url] = line.strip()\n",
    "                status = 1\n",
    "            else:\n",
    "                status = 0\n",
    "                url = \"\"\n",
    "        elif status == 1 and re.search(\"^HTTP\", line):\n",
    "            msgs[url] = line.strip()+\" \"+msgs[url]\n",
    "    logfile.close()\n",
    "    return(msgs)\n",
    "\n",
    "msgs = find_cause_of_missing_files(urls)\n",
    "print(\"missing urls in logfile:\",len(msgs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cause_of_missing_files(msgs, fetch_web_pages = False):\n",
    "    counter_404_error = 0\n",
    "    counter_500_error = 0\n",
    "\n",
    "    for url in msgs:\n",
    "        if re.search(\"404 Not Found\", msgs[url]):\n",
    "            counter_404_error += 1\n",
    "        elif re.search(\"500 Internal Server Error\", msgs[url]):\n",
    "            counter_500_error += 1\n",
    "        else:\n",
    "            print(msgs[url])\n",
    "            if fetch_web_pages:\n",
    "                print(\"fetching\", BASE_URL+url)\n",
    "                time.sleep(2)\n",
    "                os.system(WGET+\" \"+'\"'+BASE_URL+url+'\"')\n",
    "\n",
    "    counter_other = len(msgs.keys())-counter_404_error-counter_500_error\n",
    "    return(counter_404_error, counter_500_error, counter_other)\n",
    "\n",
    "counter_404_error, counter_500_error, counter_other = count_cause_of_missing_files(msgs)\n",
    "print(f\"cause of missing files: 404 error (file not found): {counter_404_error}; 500 error (access denied): {counter_500_error}; other: {counter_other}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_web_pages = True\n",
    "\n",
    "def find_missing_files_without_cause(urls, msgs, fetch_web_pages = False):\n",
    "    list_without_cause = []\n",
    "    for url in urls:\n",
    "        if not url in msgs:\n",
    "            list_without_cause.append(url)\n",
    "            if fetch_web_pages:\n",
    "                print(\"fetching\", BASE_URL+url)\n",
    "                time.sleep(2)\n",
    "                os.system(WGET+\" \"+'\"'+BASE_URL+url+'\"')\n",
    "    return(list_without_cause)\n",
    "\n",
    "print(f\"Processed: {len(find_missing_files_without_cause(urls, msgs, fetch_web_pages=fetch_web_pages))} urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_web_pages(web_pages, rename_web_pages = False):\n",
    "    file_counter = 0\n",
    "    missing_recommendations = []\n",
    "    urls = []\n",
    "    counter = 0\n",
    "    for original_url in web_pages:\n",
    "        url = urllib.parse.unquote(original_url)\n",
    "        url = re.sub(r'\\t', '%09', url)\n",
    "        if not os.path.isfile(BASE_DIR+url) and not os.path.isdir(BASE_DIR+url) and os.path.isfile((BASE_DIR+url)[:244]):\n",
    "            counter += 1\n",
    "            path_parts = (BASE_DIR+url)[:244].split(\"/\")\n",
    "            file_name_clipped = path_parts.pop(-1)\n",
    "            directory = \"/\".join(path_parts)\n",
    "            file_name_complete = url.split(\"/\")[-1]\n",
    "            if rename_web_pages:\n",
    "                os.system(f\"mv {directory}/{file_name_clipped} {directory}/{file_name_complete}\")\n",
    "    print(f\"found {counter} relevant webpages\")\n",
    "                \n",
    "rename_web_pages(web_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
