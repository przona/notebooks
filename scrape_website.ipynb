{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from colorama import Fore\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(url):\n",
    "    time.sleep(1)\n",
    "    web_page = requests.get(url)\n",
    "    if web_page.status_code == 200:\n",
    "        print(f\"retrieved web page {url} (200/{len(web_page.content)})\")\n",
    "    else:\n",
    "        print(Fore.RED, f\"web page {url} returned status code {web_page.status_code}\")\n",
    "    return(web_page.content)\n",
    "\n",
    "\n",
    "\n",
    "def get_page_links(web_page, patterns=[]):\n",
    "    page_links = []\n",
    "    for a in BeautifulSoup(web_page, \"html.parser\").select('a'):\n",
    "        try:\n",
    "            href = a.get(\"href\")\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, href):\n",
    "                    page_links.append(href)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return(page_links)\n",
    "\n",
    "\n",
    "def get_web_pages(url, patterns=[]):\n",
    "    web_page = get_web_page(url)\n",
    "    target_urls = get_page_links(web_page, patterns)\n",
    "    base_url = \"/\".join(url.split(\"/\")[0:3])\n",
    "    web_pages = {}\n",
    "    retrieved_urls = []\n",
    "    while len(set(target_urls)) > len(web_pages):\n",
    "        target_url = list(set(target_urls).difference(set(web_pages.keys())))[0]\n",
    "        if not re.search(\"\\.html$\",target_url) and not re.search(\"/[^.]*$\",target_url):\n",
    "            web_pages[target_url] = \"SKIPPED\"\n",
    "            print(f\"skipped {target_url}\")\n",
    "        elif re.search(\"/gerelateerde_documenten/\", target_url) and \\\n",
    "             \"/\".join(target_url.split(\"/\")[6:]) in retrieved_urls:\n",
    "            web_pages[target_url] = \"DUPLICATE\"\n",
    "            print(f\"duplicate {target_url}\")\n",
    "        else:\n",
    "            web_pages[target_url] = get_web_page(base_url+target_url)\n",
    "            target_urls.extend(get_page_links(web_pages[target_url], patterns))\n",
    "            if re.search(\"/gerelateerde_documenten/\", target_url):\n",
    "                retrieved_urls.append(\"/\".join(target_url.split(\"/\")[6:]))\n",
    "    return(web_pages)\n",
    "\n",
    "\n",
    "def get_recommendation_list(web_pages):\n",
    "    recommendation_list = []\n",
    "    for key in web_pages:\n",
    "        for a in BeautifulSoup(web_pages[key], \"html.parser\").select('a'):\n",
    "            try:\n",
    "                href = a.get(\"href\")\n",
    "                if re.search(\"^/richtlijn/\", href):\n",
    "                    recommendation = href.split(\"/\")[2]\n",
    "                    if recommendation not in recommendation_list:\n",
    "                        recommendation_list.append(recommendation)\n",
    "            except TypeError:\n",
    "                pass\n",
    "    return(recommendation_list)\n",
    "\n",
    "\n",
    "def save_dict(dictionary, out_file_name, mode=\"w\"):\n",
    "    pd.DataFrame(dictionary, index=[0]).T.to_csv(out_file_name, header=False, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://richtlijnendatabase.nl\"\n",
    "\n",
    "main_web_pages = get_web_pages(BASE_URL, patterns=[\"^/\\?page=\\d+$\"])\n",
    "save_dict(main_web_pages, \"main_web_pages.csv\")\n",
    "print(f\"number of pages: {len(main_web_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_list = get_recommendation_list(main_web_pages)\n",
    "print(f\"found {len(recommendation_list)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommendation in recommendation_list:\n",
    "    recommendation_web_pages = get_web_pages(BASE_URL+\"/richtlijn/\"+recommendation,\n",
    "                                             patterns=[\"^/richtlijn/\", \"^/gerelateerde_documenten\"])\n",
    "    save_dict(recommendation_web_pages, \"recommendation_web_pages.csv\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
