{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHG\n",
    "\n",
    "Website: https://richtlijnen.nhg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import pandas as pd\n",
    "import przona\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../data/richtlijnen.nhg.org/\"\n",
    "MAIN_TOPICS = [ \"behandelrichtlijnen\", \"landelijke-eerstelijns-samenwerkingsafspraken\", \"lesa\", \"landelijke-transmurale-afspraken\",\n",
    "                 \"medisch-inhoudelijke-nhg-standpunten\", \"multidisciplinaire-richtlijnen\", \"standaarden\" ]\n",
    "OTHER_TOPICS = [ \"lacunes\", \"onderzoeken\" ]\n",
    "TOPICS = sorted(list(MAIN_TOPICS) + list(OTHER_TOPICS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(file_name):\n",
    "    try:\n",
    "        infile = open(file_name)\n",
    "        text = \"\"\n",
    "        for line in infile:\n",
    "            text += line\n",
    "        infile.close()\n",
    "        return(text)\n",
    "    except Exception as e:\n",
    "        sys.exit(f\"get_text_from_file: failed processing file {file_name}\")\n",
    "        \n",
    "\n",
    "def get_codes():\n",
    "    text = get_text_from_file(BASE_DIR + \"index.html\")\n",
    "    soup = BeautifulSoup(text)\n",
    "    codes = {}\n",
    "    for tag_a in soup.findAll(\"a\"):\n",
    "        for tag_dl in tag_a.findAll(\"dl\"):\n",
    "            tags = tag_dl.findAll()\n",
    "            for i in range(0, len(tags)):\n",
    "                if tags[i].name == \"dt\" and tags[i].text == \"KNR nummer\":\n",
    "                    codes[tag_a.attrs[\"href\"]] = tags[i+1].text\n",
    "    return(codes)\n",
    "\n",
    "\n",
    "def get_subtopics(topic_name):\n",
    "    topic_dir = BASE_DIR + topic_name + \"/\"\n",
    "    file_names = sorted(os.listdir(topic_dir))\n",
    "    subtopics = []\n",
    "    for file_name in file_names:\n",
    "        if os.path.isfile(topic_dir + file_name):\n",
    "            subtopics.append(file_name)\n",
    "        elif os.path.isdir(topic_dir + file_name) and os.path.isfile(topic_dir + file_name + \"/print\"):\n",
    "            subtopics.append(file_name)\n",
    "    return(subtopics)\n",
    "\n",
    "\n",
    "def get_paragraphs(file_names, prefix):\n",
    "    base_dir = BASE_DIR + prefix + \"/\"\n",
    "    paragraphs = {}\n",
    "    for file_name in file_names:\n",
    "        file_name_with_directory = base_dir + file_name\n",
    "        if os.path.isdir(file_name_with_directory):\n",
    "            file_name_with_directory += \"/print\"\n",
    "        text = get_text_from_file(file_name_with_directory)\n",
    "        soup = BeautifulSoup(text)\n",
    "        paragraphs[\"/\" + prefix + \"/\" + file_name] = []\n",
    "        for tag in soup.findAll(\"div\", attrs={\"paragraph\", \"text-formatted\"}):\n",
    "            if len(tag.findAll(\"div\", attrs={\"paragraph\", \"text-formatted\"})) == 0:\n",
    "                paragraphs[\"/\" + prefix + \"/\" + file_name].append(tag.text)\n",
    "    return(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = get_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standaarden\n"
     ]
    }
   ],
   "source": [
    "subtopics = {}\n",
    "content_per_topic = {}\n",
    "for topic in TOPICS:\n",
    "    przona.squeal(topic)\n",
    "    subtopics[topic] = get_subtopics(topic)\n",
    "    content_per_topic[topic] = get_paragraphs(subtopics[topic], prefix=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behandelrichtlijnen 46 46\n",
      "lacunes 538 538\n",
      "landelijke-eerstelijns-samenwerkingsafspraken 11 11\n",
      "landelijke-transmurale-afspraken 5 5\n",
      "lesa 1 1\n",
      "medisch-inhoudelijke-nhg-standpunten 18 18\n",
      "multidisciplinaire-richtlijnen 51 51\n",
      "onderzoeken 302 302\n",
      "standaarden 90 90\n",
      "codes 90\n"
     ]
    }
   ],
   "source": [
    "for topic in TOPICS:\n",
    "    print(topic, len(subtopics[topic]), len(content_per_topic[topic]))\n",
    "print(\"codes\", len(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = {}\n",
    "for topic in TOPICS:\n",
    "    paragraphs.update(dict(content_per_topic[topic]))\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "przona.save_dict(paragraphs, \"csv/paragraphs_nhg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = przona.read_dict(\"csv/paragraphs_nhg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEYWORDS_FILE = \"../data/210119 Digitale zorg - sleutelwoorden en combinaties.csv\"\n",
    "\n",
    "def get_keywords(keywords_file):\n",
    "    infile = open(keywords_file, \"r\")\n",
    "    csvreader = csv.reader(infile)\n",
    "    keywords = []\n",
    "    for row in csvreader:\n",
    "        if row[1].strip() == \"ja\":\n",
    "            keywords.append(row[0].strip())\n",
    "        elif row[1].strip() == \"nee\":\n",
    "            for i in range(2, len(row)):\n",
    "                if row[i].strip() != \"\":\n",
    "                    keywords.append(row[i].strip())\n",
    "    infile.close()\n",
    "    return(keywords)\n",
    "\n",
    "def make_new_keywords(prefix, suffix_list):\n",
    "    if len(suffix_list) > 1:\n",
    "        suffix_list = make_new_keywords(suffix_list[0], suffix_list[1:])\n",
    "    new_keywords = []\n",
    "    for suffix in suffix_list:\n",
    "        new_keywords.append(prefix+suffix)\n",
    "        new_keywords.append(prefix+\" \"+suffix)\n",
    "        new_keywords.append(prefix+\"-\"+suffix)\n",
    "    return(new_keywords)\n",
    "\n",
    "def expand_keywords(keywords):\n",
    "    keywords.extend([\"e health\", \"e health toepassing\", \"e learning\", \"m health\", \n",
    "                     \"tele begeleiding\", \"tele consultatie\", \"tele health\", \"tele medicine\", \"tele monitoring\"])\n",
    "    for i in range(0,len(keywords)):\n",
    "        keywords[i] = keywords[i].strip().lower()\n",
    "    new_keywords = []\n",
    "    for keyword in keywords:\n",
    "        keyword_parts = keyword.split()\n",
    "        if len(keyword_parts) > 1:\n",
    "            for new_keyword in make_new_keywords(keyword_parts[0], keyword_parts[1:]):\n",
    "                if new_keyword not in keywords and new_keyword not in new_keywords:\n",
    "                    new_keywords.append(new_keyword)\n",
    "    keywords.extend(new_keywords)\n",
    "    return(sorted(list(set(keywords))))\n",
    "\n",
    "len(get_keywords(KEYWORDS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = expand_keywords(get_keywords(KEYWORDS_FILE))\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDATION = \"richtlijn\"\n",
    "RECOMMENDATIONS = \"richtlijnen\"\n",
    "KEYWORDS = \"sleuteltermen\"\n",
    "TERM_COUNT = \"aantal gevonden sleuteltermen\"\n",
    "TOKEN_COUNT = \"aantal tokens\"\n",
    "TYPE_COUNT = \"aantal types\"\n",
    "DOCUMENTS = \"documenten\"\n",
    "DOCUMENT = \"document\"\n",
    "PARAGRAPH = \"paragraaf\"\n",
    "PARAGRAPHS = \"paragrafen\"\n",
    "BESTPARAGRAPH = \"exemplarische paragraaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NBR_OF_WORDS = 10\n",
    "\n",
    "def count_words(text):\n",
    "    return(len(text.strip().split()))\n",
    "\n",
    "\n",
    "def remove_duplicates(matches, queries):\n",
    "    parts = []\n",
    "    wholes = []\n",
    "    for i in range(0, len(matches)):\n",
    "        for j in range(0, len(matches)):\n",
    "            if len(matches[i]) > len(matches[j]) and matches[i].startswith(matches[j]) and j not in parts and i not in wholes:\n",
    "                parts.append(j)\n",
    "                wholes.append(i)\n",
    "    for part in [p for p in sorted(parts, reverse=True)]:\n",
    "        del(matches[part])\n",
    "        del(queries[part])\n",
    "    return(matches, queries)\n",
    "\n",
    "\n",
    "def find_matching_paragraphs(keywords, paragraphs):\n",
    "    matching_paragraphs = []\n",
    "    counter = 0\n",
    "    for url in paragraphs:\n",
    "        for paragraph in paragraphs[url]:\n",
    "            if count_words(paragraph) >= MIN_NBR_OF_WORDS:\n",
    "                matches = []\n",
    "                queries = []\n",
    "                for keyword in keywords:\n",
    "                    if keyword in [\"app\", \"apps\"]:\n",
    "                        keyword = r'\\b'+keyword+r'\\b'\n",
    "                    if re.search(\"^[em] \", keyword):\n",
    "                        keyword = r'\\b'+keyword\n",
    "                    for match in re.findall(keyword, url + \" \" + paragraph, flags=re.IGNORECASE):\n",
    "                        matches.append(match)\n",
    "                        queries.append(keyword)\n",
    "                if len(matches) > 0:\n",
    "                    matches, queries = remove_duplicates(matches, queries)\n",
    "                    matching_paragraphs.append({\"url\": url, \"matches\": matches, \"queries\": queries, \"paragraph\": paragraph})\n",
    "        counter += 1\n",
    "        przona.squeal(f\"documents: {counter}; matches: {len(matching_paragraphs)}\")\n",
    "    return(matching_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 1062; matches: 83\n"
     ]
    }
   ],
   "source": [
    "matching_paragraphs = find_matching_paragraphs(keywords, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bold(text_in, queries, matches, keyword=None):\n",
    "    text_out = text_in\n",
    "    seen = {}\n",
    "    if keyword == None:\n",
    "        selected_matches = matches\n",
    "        selected_queries = queries\n",
    "    else:\n",
    "        selected_matches = [matches[i] for i in range(0, len(matches)) if normalize_keyword(matches[i]) == normalize_keyword(keyword)]\n",
    "        selected_queries = [queries[i] for i in range(0, len(matches)) if normalize_keyword(matches[i]) == normalize_keyword(keyword)]     \n",
    "    matches_by_length = [selected_matches[i] for i in sorted(range(0, len(selected_matches)), key=lambda i:len(selected_matches[i]), reverse=True)]\n",
    "    queries_by_length = [selected_queries[i] for i in sorted(range(0, len(selected_matches)), key=lambda i:len(selected_matches[i]), reverse=True)]\n",
    "    for i in range(0, len(matches_by_length)):\n",
    "        if queries_by_length[i] not in seen:\n",
    "            text_out = re.sub(\"(\"+queries_by_length[i]+\")\", \"<strong>\"+r'\\1'+\"</strong>\", text_out, flags=re.IGNORECASE)\n",
    "            seen[queries_by_length[i]] = matches_by_length[i].lower()\n",
    "        elif seen[queries_by_length[i]] != matches_by_length[i].lower():\n",
    "            print(f\"warning: make_bold: replacement problem: {seen[queries_by_length[i]]} vs {matches_by_length[i]}\")\n",
    "    return(text_out)\n",
    "\n",
    "\n",
    "def update_best_paragraph(best_paragraph, new_paragraph, recommendation, file_name, keyword=None):\n",
    "    if keyword != None:\n",
    "        matches = [k for k in new_paragraph[\"matches\"] if k == keyword]\n",
    "    else:\n",
    "        matches = new_paragraph[\"matches\"]\n",
    "    count = len(matches)\n",
    "    if TERM_COUNT not in best_paragraph or count > best_paragraph[TERM_COUNT]:\n",
    "        best_paragraph[TERM_COUNT] = count\n",
    "        best_paragraph[RECOMMENDATION] = make_bold(recommendation, new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[DOCUMENT] = make_bold(file_name, new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[PARAGRAPH] = make_bold(new_paragraph[\"paragraph\"], new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[KEYWORDS] = \" | \".join(matches)\n",
    "    return(best_paragraph)\n",
    "\n",
    "\n",
    "def normalize_keyword(keyword):\n",
    "    return(re.sub(\"[ -]\", \"\", keyword.lower()))\n",
    "\n",
    "\n",
    "def sort_and_label(dictionary_in):\n",
    "    dictionary_out = {}\n",
    "    for key_in in sorted(dictionary_in.keys(), key=lambda k:dictionary_in[k][TERM_COUNT], reverse=True):\n",
    "        key_out = f\"({dictionary_in[key_in][TERM_COUNT]}) {key_in}\"\n",
    "        if \"code\" in dictionary_in[key_in]:\n",
    "            key_out += f' ({dictionary_in[key_in][\"code\"]})'\n",
    "        dictionary_out[key_out] = dict(dictionary_in[key_in])\n",
    "    return(dictionary_out)\n",
    "\n",
    "\n",
    "def make_paragraph_number(number=None):\n",
    "    if number == None:\n",
    "        return(0)\n",
    "    return(number+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matching_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = { TERM_COUNT: 0, TOKEN_COUNT:0, TYPE_COUNT:0, BESTPARAGRAPH: {}, RECOMMENDATIONS: {}, KEYWORDS: {} }\n",
    "last_par = make_paragraph_number()\n",
    "for paragraph_data in matching_paragraphs:\n",
    "    url = paragraph_data[\"url\"]\n",
    "    if len(url.split(\"/\")) > 3:\n",
    "        recommendation = url.split(\"/\")[2]\n",
    "        filename = \"/\".join(url.split(\"/\")[3:])\n",
    "    else:\n",
    "        recommendation = url\n",
    "        filename = url\n",
    "    paragraph = make_bold(url+\" \"+paragraph_data[\"paragraph\"], paragraph_data[\"queries\"], paragraph_data[\"matches\"])\n",
    "    count_all_matches = len(paragraph_data[\"matches\"])\n",
    "    summary_data[TERM_COUNT] += count_all_matches\n",
    "    summary_data[BESTPARAGRAPH] = update_best_paragraph(summary_data[BESTPARAGRAPH], paragraph_data, recommendation, filename)\n",
    "    \n",
    "    if recommendation not in summary_data[RECOMMENDATIONS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation] = { TERM_COUNT: 0,\n",
    "                                                          TOKEN_COUNT: 0,\n",
    "                                                          TYPE_COUNT: 0,\n",
    "                                                          BESTPARAGRAPH: {},\n",
    "                                                          KEYWORDS: {},\n",
    "                                                          DOCUMENTS: {}\n",
    "                                                        }\n",
    "        if re.search(r'^/standaarden', recommendation):\n",
    "            summary_data[RECOMMENDATIONS][recommendation][\"code\"] = codes[recommendation]\n",
    "    summary_data[RECOMMENDATIONS][recommendation][TERM_COUNT] += count_all_matches\n",
    "    summary_data[RECOMMENDATIONS][recommendation][BESTPARAGRAPH] = \\\n",
    "        update_best_paragraph(summary_data[RECOMMENDATIONS][recommendation][BESTPARAGRAPH], paragraph_data, recommendation, filename)\n",
    "    if filename not in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TERM_COUNT] += count_all_matches\n",
    "    last_par = make_paragraph_number(last_par)\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "        { TERM_COUNT: count_all_matches, PARAGRAPH: paragraph }\n",
    "\n",
    "    seen = {}\n",
    "    for i in range(0, len(paragraph_data[\"queries\"])):\n",
    "        if paragraph_data[\"queries\"][i] in seen:\n",
    "            continue\n",
    "        seen[paragraph_data[\"queries\"][i]] = True\n",
    "        keyword = paragraph_data[\"matches\"][i]\n",
    "        matches = [k for k in paragraph_data[\"matches\"] if k == keyword]\n",
    "        count = len(matches)\n",
    "        normalized_keyword = normalize_keyword(keyword)\n",
    "        paragraph = make_bold(paragraph_data[\"paragraph\"], \n",
    "                              [ paragraph_data[\"queries\"][i] \n",
    "                                               for i in range(0, len(paragraph_data[\"matches\"])) \n",
    "                                               if paragraph_data[\"matches\"][i] == keyword ], \n",
    "                              [ paragraph_data[\"matches\"][i] \n",
    "                                               for i in range(0, len(paragraph_data[\"matches\"])) \n",
    "                                               if paragraph_data[\"matches\"][i] == keyword ])\n",
    "\n",
    "        if normalized_keyword not in summary_data[KEYWORDS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword] = { TERM_COUNT: 0,\n",
    "                                                           BESTPARAGRAPH: {},\n",
    "                                                           RECOMMENDATIONS: {}\n",
    "                                                         }\n",
    "        summary_data[KEYWORDS][normalized_keyword][TERM_COUNT] += count\n",
    "        summary_data[KEYWORDS][normalized_keyword][BESTPARAGRAPH] = \\\n",
    "            update_best_paragraph(summary_data[KEYWORDS][normalized_keyword][BESTPARAGRAPH], paragraph_data, recommendation, filename, keyword=keyword)\n",
    "        if recommendation not in summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, DOCUMENTS: {} }\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][TERM_COUNT] += count\n",
    "        if filename not in summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TERM_COUNT] += count\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "            { TERM_COUNT: count, DOCUMENT: make_bold(filename, paragraph_data[\"queries\"], paragraph_data[\"matches\"], keyword), PARAGRAPH: paragraph }\n",
    "        \n",
    "        if normalized_keyword not in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword] = { TERM_COUNT: 0, DOCUMENTS: {} }\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][TERM_COUNT] += count\n",
    "        if filename not in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename][TERM_COUNT] += count\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "            { TERM_COUNT: count, DOCUMENT: make_bold(filename, paragraph_data[\"queries\"], paragraph_data[\"matches\"], keyword), PARAGRAPH: paragraph }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_paragraph(paragraph_in):\n",
    "    paragraph_out = []\n",
    "    stop_words = stopwords.words('dutch')\n",
    "    for token in paragraph_in.lower().split():\n",
    "        if re.search(r\"^[-a-zÞ-ÿ']+$\", token) and token not in stop_words:\n",
    "            paragraph_out.append(token)\n",
    "    return(\" \".join(paragraph_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_paragraph_list(paragraph_list):\n",
    "    token_count = 0\n",
    "    token_set = set()\n",
    "    for paragraph in paragraph_list:\n",
    "        paragraph = cleanup_paragraph(paragraph)\n",
    "        token_count += len(paragraph.split())\n",
    "        token_set = token_set.union(set(paragraph.lower().split()))\n",
    "    return(token_count, token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "token_set_all = set()\n",
    "recommendation_data = {}\n",
    "for url in paragraphs:\n",
    "    if True:\n",
    "        counter += 1\n",
    "        przona.squeal(counter)\n",
    "        token_count, token_set = count_tokens_in_paragraph_list(paragraphs[url])\n",
    "        summary_data[TOKEN_COUNT] += token_count\n",
    "        token_set_all = token_set_all.union(token_set)\n",
    "        if len(url.split(\"/\")) > 3:\n",
    "            recommendation = url.split(\"/\")[2]\n",
    "            filename = \"/\".join(url.split(\"/\")[3:])\n",
    "        else:\n",
    "            recommendation = url\n",
    "            filename = url\n",
    "        if recommendation not in recommendation_data:\n",
    "            recommendation_data[recommendation] = { TOKEN_COUNT: 0, TYPE_COUNT: 0 }\n",
    "        recommendation_data[recommendation][TOKEN_COUNT] += token_count\n",
    "        recommendation_data[recommendation][TYPE_COUNT] += len(token_set)\n",
    "        if recommendation in summary_data[RECOMMENDATIONS]:\n",
    "            if filename in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "                summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TOKEN_COUNT] = token_count\n",
    "                summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TYPE_COUNT] = len(token_set)\n",
    "                for keyword in summary_data[KEYWORDS].keys():\n",
    "                    if recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "                        if filename in summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "                            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TOKEN_COUNT] = token_count\n",
    "                            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TYPE_COUNT] = len(token_set)\n",
    "summary_data[TYPE_COUNT] += len(token_set_all)\n",
    "for recommendation in recommendation_data:\n",
    "    if recommendation in summary_data[RECOMMENDATIONS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][TOKEN_COUNT] = recommendation_data[recommendation][TOKEN_COUNT]\n",
    "        summary_data[RECOMMENDATIONS][recommendation][TYPE_COUNT] = recommendation_data[recommendation][TYPE_COUNT]\n",
    "    for keyword in summary_data[KEYWORDS].keys():\n",
    "        if recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][TOKEN_COUNT] = recommendation_data[recommendation][TOKEN_COUNT]\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][TYPE_COUNT] = recommendation_data[recommendation][TYPE_COUNT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side track: make csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "for topic in sorted(subtopics.keys()):\n",
    "    for subtopic in sorted(subtopics[topic]):\n",
    "        code = \"\"\n",
    "        url = f\"/{topic}/{subtopic}\" \n",
    "        if url in codes:\n",
    "            code = codes[url]\n",
    "        token_count, token_set = count_tokens_in_paragraph_list(paragraphs[url])\n",
    "        keyword_count = \"\"\n",
    "        if url in summary_data[RECOMMENDATIONS]:\n",
    "            keyword_count = summary_data[RECOMMENDATIONS][url][TERM_COUNT]\n",
    "        data_table.append({\"categorie\": topic, \"onderwerp\": subtopic, \"code\": code, \"tokens\": token_count, \"types\": len(token_set), \"sleutelwoorden\": keyword_count})\n",
    "pd.DataFrame(data_table).to_csv(\"csv/nhg_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continue with building json and html files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data[RECOMMENDATIONS] = sort_and_label(summary_data[RECOMMENDATIONS])\n",
    "summary_data[KEYWORDS] = sort_and_label(summary_data[KEYWORDS])\n",
    "for recommendation in summary_data[RECOMMENDATIONS]:\n",
    "    summary_data[RECOMMENDATIONS][recommendation][KEYWORDS] = sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS])\n",
    "    for keyword in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS] = \\\n",
    "            sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS])\n",
    "        for url in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS][url][PARAGRAPHS] = \\\n",
    "                sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS][url][PARAGRAPHS])\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS] = sort_and_label(summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS])\n",
    "    for filename in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS] = \\\n",
    "            sort_and_label(summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS])\n",
    "for keyword in summary_data[KEYWORDS]:\n",
    "    summary_data[KEYWORDS][keyword][RECOMMENDATIONS] = sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS])\n",
    "    for recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "        summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS] = \\\n",
    "            sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS])\n",
    "        for url in summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][url][PARAGRAPHS] = \\\n",
    "                sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][url][PARAGRAPHS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_large_number_readable(number_in):\n",
    "    number_in = str(number_in)\n",
    "    number_out = \"\"\n",
    "    for i in range(1, 1+len(number_in)):\n",
    "        number_out = number_in[-i] + number_out\n",
    "        if i % 3 == 0 and i < len(number_in):\n",
    "            number_out = \".\" + number_out\n",
    "    return(number_out)\n",
    "\n",
    "\n",
    "def make_large_numbers_readable(dict_):\n",
    "    for key in dict_:\n",
    "        if isinstance(dict_[key], int):\n",
    "            dict_[key] = make_large_number_readable(dict_[key])\n",
    "        elif isinstance(dict_[key], dict):\n",
    "            dict_[key] = make_large_numbers_readable(dict_[key])\n",
    "    return(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = make_large_numbers_readable(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in summary_data[RECOMMENDATIONS]:\n",
    "    topic = key.split(\"/\")[1]\n",
    "    if topic not in summary_data:\n",
    "        summary_data[topic] = {}\n",
    "    summary_data[topic][key] = dict(summary_data[RECOMMENDATIONS][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(summary_data[RECOMMENDATIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"paragraphs.json\", \"w\")\n",
    "print(json.dumps(summary_data), file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2html(data, outfile, top=False, counter=0):\n",
    "    if type(data) != dict:\n",
    "        print(f\"<font style=\\\"color:grey;\\\">{data}</font>\", file=outfile)\n",
    "    else:\n",
    "        if top:\n",
    "            print(f\"<div id=\\\"div{counter}\\\" style=\\\"display:block\\\">\\n<ul>\", file=outfile)\n",
    "        else:\n",
    "            print(f\"<a href=\\\"javascript:toggle('div{counter}')\\\" id=\\\"div{counter}link\\\">open</a>\", file=outfile)\n",
    "            print(f\"<div id=\\\"div{counter}\\\" style=\\\"display:none\\\">\\n<ul>\", file=outfile)\n",
    "        for key in data:\n",
    "            print(\"<li>\", key, \":\", file=outfile)\n",
    "            counter += 1\n",
    "            counter = json2html(data[key], outfile, counter=counter)\n",
    "            print(\"</li>\", file=outfile)\n",
    "        print(\"</ul>\\n</div>\", file=outfile)\n",
    "    return(counter)\n",
    "\n",
    "title = \"analyse richtlijnen.nhg.nl\"\n",
    "outfile = open(\"index.html\", \"w\")\n",
    "print(f\"<html>\\n<head>\\n<meta charset=\\\"utf-8\\\"/>\\n<title>{title}</title>\\n<script type=\\\"text/javascript\\\">\", file=outfile)\n",
    "print(\"function toggle(divid) {\\nvar item=document.getElementById(divid); if (item) { item.style.display=(item.style.display=='none')?'block':'none'; }\\n\"\n",
    "      \"var itemlink=document.getElementById(divid+'link'); if (itemlink) { itemlink.text=(itemlink.text=='open')?'sluit':'open'; }}\", file=outfile)\n",
    "print(f\"</script>\\n</head>\\n<body><h2>{title}</h2>\", file=outfile)\n",
    "json2html(summary_data, outfile, top=True)\n",
    "print(\"</body>\\n</html>\", file=outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
