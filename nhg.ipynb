{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import przona\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../data/richtlijnen.nhg.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standards():\n",
    "    standards_dir = BASE_DIR + \"standaarden\" + \"/\"\n",
    "    file_names = sorted(os.listdir(standards_dir))\n",
    "    standards = []\n",
    "    for directory in file_names:\n",
    "        if os.path.isdir(standards_dir + directory) and os.path.isfile(standards_dir + directory + \"/print\"):\n",
    "            standards.append(directory)\n",
    "    return(standards)\n",
    "\n",
    "def get_omissions(dir_name):\n",
    "    omissions_dir = BASE_DIR + dir_name + \"/\"\n",
    "    file_names = sorted(os.listdir(omissions_dir))\n",
    "    omissions = []\n",
    "    for file_name in file_names:\n",
    "        if os.path.isfile(omissions_dir + file_name):\n",
    "            omissions.append(file_name)\n",
    "    return(omissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(file_names, prefix):\n",
    "    base_dir = BASE_DIR + prefix + \"/\"\n",
    "    paragraphs = {}\n",
    "    for file_name in file_names:\n",
    "        file_name_with_directory = base_dir + file_name\n",
    "        if prefix == \"standaarden\":\n",
    "            file_name_with_directory += \"/print\"\n",
    "        infile = open(file_name_with_directory)\n",
    "        text = \"\"\n",
    "        for line in infile:\n",
    "            text += line\n",
    "        infile.close()\n",
    "        soup = BeautifulSoup(text)\n",
    "        paragraphs[\"/\" + prefix + \"/\" + file_name] = []\n",
    "        for tag in soup.findAll(\"div\", attrs={\"paragraph\", \"text-formatted\"}):\n",
    "            if len(tag.findAll(\"div\", attrs={\"paragraph\", \"text-formatted\"})) == 0:\n",
    "                paragraphs[\"/\" + prefix + \"/\" + file_name].append(tag.text)\n",
    "    return(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "standards = get_standards()\n",
    "paragraphs_standards = get_paragraphs(standards, prefix=\"standaarden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "omissions = get_omissions(\"lacunes\")\n",
    "paragraphs_omissions = get_paragraphs(omissions, prefix=\"lacunes\")\n",
    "studies = get_omissions(\"onderzoeken\")\n",
    "paragraphs_studies = get_paragraphs(studies, prefix=\"onderzoeken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 538, 538, 302, 302)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(standards), len(paragraphs_standards), len(omissions), len(paragraphs_omissions), len(studies), len(paragraphs_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = dict(paragraphs_standards)\n",
    "paragraphs.update(dict(paragraphs_omissions))\n",
    "paragraphs.update(dict(paragraphs_studies))\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "przona.save_dict(paragraphs, \"csv/paragraphs_nhg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = przona.read_dict(\"csv/paragraphs_nhg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEYWORDS_FILE = \"../data/210119 Digitale zorg - sleutelwoorden en combinaties.csv\"\n",
    "\n",
    "def get_keywords(keywords_file):\n",
    "    infile = open(keywords_file, \"r\")\n",
    "    csvreader = csv.reader(infile)\n",
    "    keywords = []\n",
    "    for row in csvreader:\n",
    "        if row[1].strip() == \"ja\":\n",
    "            keywords.append(row[0].strip())\n",
    "        elif row[1].strip() == \"nee\":\n",
    "            for i in range(2, len(row)):\n",
    "                if row[i].strip() != \"\":\n",
    "                    keywords.append(row[i].strip())\n",
    "    infile.close()\n",
    "    return(keywords)\n",
    "\n",
    "def make_new_keywords(prefix, suffix_list):\n",
    "    if len(suffix_list) > 1:\n",
    "        suffix_list = make_new_keywords(suffix_list[0], suffix_list[1:])\n",
    "    new_keywords = []\n",
    "    for suffix in suffix_list:\n",
    "        new_keywords.append(prefix+suffix)\n",
    "        new_keywords.append(prefix+\" \"+suffix)\n",
    "        new_keywords.append(prefix+\"-\"+suffix)\n",
    "    return(new_keywords)\n",
    "\n",
    "def expand_keywords(keywords):\n",
    "    keywords.extend([\"e health\", \"e health toepassing\", \"e learning\", \"m health\", \n",
    "                     \"tele begeleiding\", \"tele consultatie\", \"tele health\", \"tele medicine\", \"tele monitoring\"])\n",
    "    for i in range(0,len(keywords)):\n",
    "        keywords[i] = keywords[i].strip().lower()\n",
    "    new_keywords = []\n",
    "    for keyword in keywords:\n",
    "        keyword_parts = keyword.split()\n",
    "        if len(keyword_parts) > 1:\n",
    "            for new_keyword in make_new_keywords(keyword_parts[0], keyword_parts[1:]):\n",
    "                if new_keyword not in keywords and new_keyword not in new_keywords:\n",
    "                    new_keywords.append(new_keyword)\n",
    "    keywords.extend(new_keywords)\n",
    "    return(sorted(list(set(keywords))))\n",
    "\n",
    "len(get_keywords(KEYWORDS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = expand_keywords(get_keywords(KEYWORDS_FILE))\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDATION = \"richtlijn\"\n",
    "RECOMMENDATIONS = \"richtlijnen\"\n",
    "KEYWORDS = \"sleuteltermen\"\n",
    "TERM_COUNT = \"aantal gevonden sleuteltermen\"\n",
    "TOKEN_COUNT = \"aantal tokens\"\n",
    "TYPE_COUNT = \"aantal types\"\n",
    "DOCUMENTS = \"documenten\"\n",
    "DOCUMENT = \"document\"\n",
    "PARAGRAPH = \"paragraaf\"\n",
    "PARAGRAPHS = \"paragrafen\"\n",
    "BESTPARAGRAPH = \"exemplarische paragraaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 931; matches: 77\n"
     ]
    }
   ],
   "source": [
    "MIN_NBR_OF_WORDS = 10\n",
    "\n",
    "def count_words(text):\n",
    "    return(len(text.strip().split()))\n",
    "\n",
    "\n",
    "def remove_duplicates(matches, queries):\n",
    "    parts = []\n",
    "    wholes = []\n",
    "    for i in range(0, len(matches)):\n",
    "        for j in range(0, len(matches)):\n",
    "            if len(matches[i]) > len(matches[j]) and matches[i].startswith(matches[j]) and j not in parts and i not in wholes:\n",
    "                parts.append(j)\n",
    "                wholes.append(i)\n",
    "    for part in [p for p in sorted(parts, reverse=True)]:\n",
    "        del(matches[part])\n",
    "        del(queries[part])\n",
    "    return(matches, queries)\n",
    "\n",
    "\n",
    "def find_matching_paragraphs(keywords, paragraphs):\n",
    "    matching_paragraphs = []\n",
    "    counter = 0\n",
    "    for url in paragraphs:\n",
    "        for paragraph in paragraphs[url]:\n",
    "            if count_words(paragraph) >= MIN_NBR_OF_WORDS:\n",
    "                matches = []\n",
    "                queries = []\n",
    "                for keyword in keywords:\n",
    "                    if keyword in [\"app\", \"apps\"]:\n",
    "                        keyword = r'\\b'+keyword+r'\\b'\n",
    "                    if re.search(\"^[em] \", keyword):\n",
    "                        keyword = r'\\b'+keyword\n",
    "                    for match in re.findall(keyword, url + \" \" + paragraph, flags=re.IGNORECASE):\n",
    "                        matches.append(match)\n",
    "                        queries.append(keyword)\n",
    "                if len(matches) > 0:\n",
    "                    matches, queries = remove_duplicates(matches, queries)\n",
    "                    matching_paragraphs.append({\"url\": url, \"matches\": matches, \"queries\": queries, \"paragraph\": paragraph})\n",
    "        counter += 1\n",
    "        przona.squeal(f\"documents: {counter}; matches: {len(matching_paragraphs)}\")\n",
    "    return(matching_paragraphs)\n",
    "\n",
    "matching_paragraphs = find_matching_paragraphs(keywords, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bold(text_in, queries, matches, keyword=None):\n",
    "    text_out = text_in\n",
    "    seen = {}\n",
    "    if keyword == None:\n",
    "        selected_matches = matches\n",
    "        selected_queries = queries\n",
    "    else:\n",
    "        selected_matches = [matches[i] for i in range(0, len(matches)) if normalize_keyword(matches[i]) == normalize_keyword(keyword)]\n",
    "        selected_queries = [queries[i] for i in range(0, len(matches)) if normalize_keyword(matches[i]) == normalize_keyword(keyword)]     \n",
    "    matches_by_length = [selected_matches[i] for i in sorted(range(0, len(selected_matches)), key=lambda i:len(selected_matches[i]), reverse=True)]\n",
    "    queries_by_length = [selected_queries[i] for i in sorted(range(0, len(selected_matches)), key=lambda i:len(selected_matches[i]), reverse=True)]\n",
    "    for i in range(0, len(matches_by_length)):\n",
    "        if queries_by_length[i] not in seen:\n",
    "            text_out = re.sub(\"(\"+queries_by_length[i]+\")\", \"<strong>\"+r'\\1'+\"</strong>\", text_out, flags=re.IGNORECASE)\n",
    "            seen[queries_by_length[i]] = matches_by_length[i].lower()\n",
    "        elif seen[queries_by_length[i]] != matches_by_length[i].lower():\n",
    "            print(f\"warning: make_bold: replacement problem: {seen[queries_by_length[i]]} vs {matches_by_length[i]}\")\n",
    "    return(text_out)\n",
    "\n",
    "\n",
    "def update_best_paragraph(best_paragraph, new_paragraph, recommendation, file_name, keyword=None):\n",
    "    if keyword != None:\n",
    "        matches = [k for k in new_paragraph[\"matches\"] if k == keyword]\n",
    "    else:\n",
    "        matches = new_paragraph[\"matches\"]\n",
    "    count = len(matches)\n",
    "    if TERM_COUNT not in best_paragraph or count > best_paragraph[TERM_COUNT]:\n",
    "        best_paragraph[TERM_COUNT] = count\n",
    "        best_paragraph[RECOMMENDATION] = make_bold(recommendation, new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[DOCUMENT] = make_bold(file_name, new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[PARAGRAPH] = make_bold(new_paragraph[\"paragraph\"], new_paragraph[\"queries\"], new_paragraph[\"matches\"], keyword=keyword)\n",
    "        best_paragraph[KEYWORDS] = \" | \".join(matches)\n",
    "    return(best_paragraph)\n",
    "\n",
    "\n",
    "def normalize_keyword(keyword):\n",
    "    return(re.sub(\"[ -]\", \"\", keyword.lower()))\n",
    "\n",
    "\n",
    "def sort_and_label(dictionary):\n",
    "    return({f\"({dictionary[key][TERM_COUNT]}) {key}\":dictionary[key] \n",
    "            for key in sorted(dictionary, key=lambda k:dictionary[k][TERM_COUNT], reverse=True)})\n",
    "\n",
    "\n",
    "def make_paragraph_number(number=None):\n",
    "    if number == None:\n",
    "        return(0)\n",
    "    return(number+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matching_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = { TERM_COUNT: 0, TOKEN_COUNT:0, TYPE_COUNT:0, BESTPARAGRAPH: {}, RECOMMENDATIONS: {}, KEYWORDS: {} }\n",
    "last_par = make_paragraph_number()\n",
    "for paragraph_data in matching_paragraphs:\n",
    "    url = paragraph_data[\"url\"]\n",
    "    if len(url.split(\"/\")) > 3:\n",
    "        recommendation = url.split(\"/\")[2]\n",
    "        filename = \"/\".join(url.split(\"/\")[3:])\n",
    "    else:\n",
    "        recommendation = url\n",
    "        filename = \"\"\n",
    "    paragraph = make_bold(url+\" \"+paragraph_data[\"paragraph\"], paragraph_data[\"queries\"], paragraph_data[\"matches\"])\n",
    "    count_all_matches = len(paragraph_data[\"matches\"])\n",
    "    summary_data[TERM_COUNT] += count_all_matches\n",
    "    summary_data[BESTPARAGRAPH] = update_best_paragraph(summary_data[BESTPARAGRAPH], paragraph_data, recommendation, filename)\n",
    "    \n",
    "    if recommendation not in summary_data[RECOMMENDATIONS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation] = { TERM_COUNT: 0,\n",
    "                                                          TOKEN_COUNT: 0,\n",
    "                                                          TYPE_COUNT: 0,\n",
    "                                                          BESTPARAGRAPH: {},\n",
    "                                                          KEYWORDS: {},\n",
    "                                                          DOCUMENTS: {}\n",
    "                                                        }\n",
    "    summary_data[RECOMMENDATIONS][recommendation][TERM_COUNT] += count_all_matches\n",
    "    summary_data[RECOMMENDATIONS][recommendation][BESTPARAGRAPH] = \\\n",
    "        update_best_paragraph(summary_data[RECOMMENDATIONS][recommendation][BESTPARAGRAPH], paragraph_data, recommendation, filename)\n",
    "    if filename not in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TERM_COUNT] += count_all_matches\n",
    "    last_par = make_paragraph_number(last_par)\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "        { TERM_COUNT: count_all_matches, PARAGRAPH: paragraph }\n",
    "\n",
    "    seen = {}\n",
    "    for i in range(0, len(paragraph_data[\"queries\"])):\n",
    "        if paragraph_data[\"queries\"][i] in seen:\n",
    "            continue\n",
    "        seen[paragraph_data[\"queries\"][i]] = True\n",
    "        keyword = paragraph_data[\"matches\"][i]\n",
    "        matches = [k for k in paragraph_data[\"matches\"] if k == keyword]\n",
    "        count = len(matches)\n",
    "        normalized_keyword = normalize_keyword(keyword)\n",
    "        paragraph = make_bold(paragraph_data[\"paragraph\"], \n",
    "                              [ paragraph_data[\"queries\"][i] \n",
    "                                               for i in range(0, len(paragraph_data[\"matches\"])) \n",
    "                                               if paragraph_data[\"matches\"][i] == keyword ], \n",
    "                              [ paragraph_data[\"matches\"][i] \n",
    "                                               for i in range(0, len(paragraph_data[\"matches\"])) \n",
    "                                               if paragraph_data[\"matches\"][i] == keyword ])\n",
    "\n",
    "        if normalized_keyword not in summary_data[KEYWORDS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword] = { TERM_COUNT: 0,\n",
    "                                                           BESTPARAGRAPH: {},\n",
    "                                                           RECOMMENDATIONS: {}\n",
    "                                                         }\n",
    "        summary_data[KEYWORDS][normalized_keyword][TERM_COUNT] += count\n",
    "        summary_data[KEYWORDS][normalized_keyword][BESTPARAGRAPH] = \\\n",
    "            update_best_paragraph(summary_data[KEYWORDS][normalized_keyword][BESTPARAGRAPH], paragraph_data, recommendation, filename, keyword=keyword)\n",
    "        if recommendation not in summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, DOCUMENTS: {} }\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][TERM_COUNT] += count\n",
    "        if filename not in summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "            summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TERM_COUNT] += count\n",
    "        summary_data[KEYWORDS][normalized_keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "            { TERM_COUNT: count, DOCUMENT: make_bold(filename, paragraph_data[\"queries\"], paragraph_data[\"matches\"], keyword), PARAGRAPH: paragraph }\n",
    "        \n",
    "        if normalized_keyword not in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword] = { TERM_COUNT: 0, DOCUMENTS: {} }\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][TERM_COUNT] += count\n",
    "        if filename not in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename] = { TERM_COUNT: 0, TOKEN_COUNT: 0, TYPE_COUNT: 0, PARAGRAPHS: {} }\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename][TERM_COUNT] += count\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][normalized_keyword][DOCUMENTS][filename][PARAGRAPHS][\"PAR\"+str(last_par)] = \\\n",
    "            { TERM_COUNT: count, DOCUMENT: make_bold(filename, paragraph_data[\"queries\"], paragraph_data[\"matches\"], keyword), PARAGRAPH: paragraph }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_paragraph(paragraph_in):\n",
    "    paragraph_out = []\n",
    "    stop_words = stopwords.words('dutch')\n",
    "    for token in paragraph_in.lower().split():\n",
    "        if re.search(r\"^[-a-zÞ-ÿ']+$\", token) and token not in stop_words:\n",
    "            paragraph_out.append(token)\n",
    "    return(\" \".join(paragraph_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "token_set_all = set()\n",
    "recommendation_data = {}\n",
    "for url in paragraphs:\n",
    "    if True:\n",
    "        przona.squeal(counter)\n",
    "        counter += 1\n",
    "        token_count = 0\n",
    "        token_set = set()\n",
    "        for paragraph in paragraphs[url]:\n",
    "            paragraph = cleanup_paragraph(paragraph)\n",
    "            token_count += len(paragraph.split())\n",
    "            token_set = token_set.union(set(paragraph.lower().split()))\n",
    "        summary_data[TOKEN_COUNT] += token_count\n",
    "        token_set_all = token_set_all.union(token_set)\n",
    "        if len(url.split(\"/\")) > 3:\n",
    "            recommendation = url.split(\"/\")[2]\n",
    "            filename = \"/\".join(url.split(\"/\")[3:])\n",
    "        else:\n",
    "            recommendation = url\n",
    "            filename = \"\"\n",
    "        if recommendation not in recommendation_data:\n",
    "            recommendation_data[recommendation] = { TOKEN_COUNT: 0, TYPE_COUNT: 0 }\n",
    "        recommendation_data[recommendation][TOKEN_COUNT] += token_count\n",
    "        recommendation_data[recommendation][TYPE_COUNT] += len(token_set)\n",
    "        if recommendation in summary_data[RECOMMENDATIONS]:\n",
    "            if filename in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "                summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TOKEN_COUNT] = token_count\n",
    "                summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TYPE_COUNT] = len(token_set)\n",
    "                for keyword in summary_data[KEYWORDS].keys():\n",
    "                    if recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "                        if filename in summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "                            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TOKEN_COUNT] = token_count\n",
    "                            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][filename][TYPE_COUNT] = len(token_set)\n",
    "summary_data[TYPE_COUNT] += len(token_set_all)\n",
    "for recommendation in recommendation_data:\n",
    "    if recommendation in summary_data[RECOMMENDATIONS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][TOKEN_COUNT] = recommendation_data[recommendation][TOKEN_COUNT]\n",
    "        summary_data[RECOMMENDATIONS][recommendation][TYPE_COUNT] = recommendation_data[recommendation][TYPE_COUNT]\n",
    "    for keyword in summary_data[KEYWORDS].keys():\n",
    "        if recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][TOKEN_COUNT] = recommendation_data[recommendation][TOKEN_COUNT]\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][TYPE_COUNT] = recommendation_data[recommendation][TYPE_COUNT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data[RECOMMENDATIONS] = sort_and_label(summary_data[RECOMMENDATIONS])\n",
    "summary_data[KEYWORDS] = sort_and_label(summary_data[KEYWORDS])\n",
    "for recommendation in summary_data[RECOMMENDATIONS]:\n",
    "    summary_data[RECOMMENDATIONS][recommendation][KEYWORDS] = sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS])\n",
    "    for keyword in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS] = \\\n",
    "            sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS])\n",
    "        for url in summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS]:\n",
    "            summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS][url][PARAGRAPHS] = \\\n",
    "                sort_and_label(summary_data[RECOMMENDATIONS][recommendation][KEYWORDS][keyword][DOCUMENTS][url][PARAGRAPHS])\n",
    "    summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS] = sort_and_label(summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS])\n",
    "    for filename in summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "        summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS] = \\\n",
    "            sort_and_label(summary_data[RECOMMENDATIONS][recommendation][DOCUMENTS][filename][PARAGRAPHS])\n",
    "for keyword in summary_data[KEYWORDS]:\n",
    "    summary_data[KEYWORDS][keyword][RECOMMENDATIONS] = sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS])\n",
    "    for recommendation in summary_data[KEYWORDS][keyword][RECOMMENDATIONS]:\n",
    "        summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS] = \\\n",
    "            sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS])\n",
    "        for url in summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS]:\n",
    "            summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][url][PARAGRAPHS] = \\\n",
    "                sort_and_label(summary_data[KEYWORDS][keyword][RECOMMENDATIONS][recommendation][DOCUMENTS][url][PARAGRAPHS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_large_number_readable(number_in):\n",
    "    number_in = str(number_in)\n",
    "    number_out = \"\"\n",
    "    for i in range(1, 1+len(number_in)):\n",
    "        number_out = number_in[-i] + number_out\n",
    "        if i % 3 == 0 and i < len(number_in):\n",
    "            number_out = \".\" + number_out\n",
    "    return(number_out)\n",
    "\n",
    "\n",
    "def make_large_numbers_readable(dict_):\n",
    "    for key in dict_:\n",
    "        if isinstance(dict_[key], int):\n",
    "            dict_[key] = make_large_number_readable(dict_[key])\n",
    "        elif isinstance(dict_[key], dict):\n",
    "            dict_[key] = make_large_numbers_readable(dict_[key])\n",
    "    return(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = make_large_numbers_readable(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(\"paragraphs.json\", \"w\")\n",
    "print(json.dumps(summary_data), file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2html(data, outfile, top=False, counter=0):\n",
    "    if type(data) != dict:\n",
    "        print(f\"<font style=\\\"color:grey;\\\">{data}</font>\", file=outfile)\n",
    "    else:\n",
    "        if top:\n",
    "            print(f\"<div id=\\\"div{counter}\\\" style=\\\"display:block\\\">\\n<ul>\", file=outfile)\n",
    "        else:\n",
    "            print(f\"<a href=\\\"javascript:toggle('div{counter}')\\\" id=\\\"div{counter}link\\\">open</a>\", file=outfile)\n",
    "            print(f\"<div id=\\\"div{counter}\\\" style=\\\"display:none\\\">\\n<ul>\", file=outfile)\n",
    "        for key in data:\n",
    "            print(\"<li>\", key, \":\", file=outfile)\n",
    "            counter += 1\n",
    "            counter = json2html(data[key], outfile, counter=counter)\n",
    "            print(\"</li>\", file=outfile)\n",
    "        print(\"</ul>\\n</div>\", file=outfile)\n",
    "    return(counter)\n",
    "\n",
    "title = \"analyse richtlijnen.nhg.nl\"\n",
    "outfile = open(\"index.html\", \"w\")\n",
    "print(f\"<html>\\n<head>\\n<meta charset=\\\"utf-8\\\"/>\\n<title>{title}</title>\\n<script type=\\\"text/javascript\\\">\", file=outfile)\n",
    "print(\"function toggle(divid) {\\nvar item=document.getElementById(divid); if (item) { item.style.display=(item.style.display=='none')?'block':'none'; }\\n\"\n",
    "      \"var itemlink=document.getElementById(divid+'link'); if (itemlink) { itemlink.text=(itemlink.text=='open')?'sluit':'open'; }}\", file=outfile)\n",
    "print(f\"</script>\\n</head>\\n<body><h2>{title}</h2>\", file=outfile)\n",
    "json2html(summary_data, outfile, top=True)\n",
    "print(\"</body>\\n</html>\", file=outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
