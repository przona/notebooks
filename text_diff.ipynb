{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text diff\n",
    "\n",
    "Find the differences in the text between two crawls of richtlijnendatabase.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paragraphs_file(file_name, gzip_flag=False):\n",
    "    if gzip_flag:\n",
    "        infile = gzip.open(file_name, mode=\"rt\")\n",
    "    else:\n",
    "        infile = open(file_name, \"r\")\n",
    "    csvreader = csv.reader(infile)\n",
    "    dict_out = {}\n",
    "    for row in csvreader:\n",
    "        key = row.pop(0)\n",
    "        dict_out[key] = row\n",
    "    infile.close()\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract paragraphs and compare\n",
    "\n",
    "Part of this code was based on the notebook `keyword_search.ipynb` section `Text segmentation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import przona\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(soup):\n",
    "    paragraphs = []\n",
    "    for paragraph in soup.find_all(re.compile(\"^(p|li|h[0-9])$\")):\n",
    "        text = re.sub(\"\\s+\", \" \", paragraph.text).strip()\n",
    "        if text != \"\":\n",
    "            paragraphs.append(text)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    file_id = open(file_name, \"r\")\n",
    "    text = \"\"\n",
    "    for line in file_id:\n",
    "        text += line\n",
    "    file_id.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_files(target_dir):\n",
    "    paragraphs = {}\n",
    "    counter = 0\n",
    "    for root, dirs, files in os.walk(target_dir):\n",
    "        for file_name in files:\n",
    "            if re.search(r'.html$', file_name):\n",
    "                counter += 1\n",
    "                if counter % 100 == 0:\n",
    "                    przona.squeal(counter)\n",
    "                file_location = os.path.join(root, file_name)\n",
    "                web_page_text = read_file(file_location)\n",
    "                web_page_text_with_spaces = re.sub(\">\", \"> \", web_page_text)\n",
    "                soup = BeautifulSoup(web_page_text_with_spaces)\n",
    "                file_location_relative = re.sub(target_dir, \"\", file_location)\n",
    "                paragraphs[file_location_relative] = get_paragraphs(soup)\n",
    "    przona.squeal(counter)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(links_in):\n",
    "    links_out = []\n",
    "    for i in range(0, len(links_in)):\n",
    "        if len(links_in[i]) > 1 and i > 0 and len(links_out[-1]) == 1 and links_out[-1][0] + 1 in links_in[i]:\n",
    "            links_out.append([links_out[-1][0] + 1])\n",
    "        else:\n",
    "            links_out.append(links_in[i])\n",
    "    for i in range(len(links_out)-1, -1, -1):\n",
    "        if len(links_out[i]) > 1 and i < len(links_out)-1 and len(links_out[i+1]) == 1 and links_out[i+1][0] - 1 in links_out[i]:\n",
    "            links_out[i] = [links_out[i+1][0] - 1]\n",
    "    return(links_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gaps(links, links_old, paragraphs1, paragraphs2):\n",
    "    text_out = \"\"\n",
    "    processed = {}\n",
    "    for i in range(0, len(links)):\n",
    "        if len(links[i]) == 0 and i > 0 and i < len(links)-1 and len(links[i-1]) == 1 and len(links[i+1]) == 1 and links[i-1][0] == links[i+1][0] - 2:\n",
    "            replacement_id = links[i - 1][0] + 1\n",
    "            processed[replacement_id] = True\n",
    "            if paragraphs1[i] != \"Log in\" or paragraphs2[replacement_id] != \"Inloggen\":\n",
    "                text_out += f\"[REPLACED] {paragraphs1[i]} [WAS] {paragraphs2[replacement_id]}\\n\"\n",
    "        elif len(links[i]) == 0 and i > 0 and i < len(links)-1 and len(links[i-1]) == 1 and len(links[i+1]) == 1 and links[i-1][0] == links[i+1][0] - 1:\n",
    "            if paragraphs1[i] != \"Zoek\":\n",
    "                text_out += f\"[INSERTED] {paragraphs1[i]}\\n\"\n",
    "        elif len(links[i]) == 0:\n",
    "            text_out += f\"[NEW] {paragraphs1[i]}\\n\"\n",
    "    for i in range(0, len(links_old)):\n",
    "        if len(links_old[i]) == 0 and i not in processed:\n",
    "            text_out += f\"[REMOVED] {paragraphs2[i]}\\n\"\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9335\n"
     ]
    }
   ],
   "source": [
    "paragraphs = process_files(\"../data/richtlijnendatabase.nl/richtlijn\")\n",
    "paragraphs_old = process_files(\"../data/richtlijnendatabase.nl-20210315/richtlijn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_name, outfile=sys.stdout):\n",
    "    links = [ [] for _ in range(0, len(paragraphs[file_name])) ]\n",
    "    links_old = [ [] for _ in range(0, len(paragraphs_old[file_name])) ]\n",
    "    for i in range(0, len(paragraphs[file_name])):\n",
    "        for j in range(0, len(paragraphs_old[file_name])):\n",
    "            if paragraphs[file_name][i] == paragraphs_old[file_name][j]:\n",
    "                links[i].append(j)\n",
    "                links_old[j].append(i)\n",
    "    text_out = find_gaps(remove_duplicates(links), remove_duplicates(links_old), paragraphs[file_name], paragraphs_old[file_name]) \n",
    "    if text_out != \"\":\n",
    "        print(f\"{file_name} [{len(paragraphs[file_name])}] [{len(paragraphs_old[file_name])}]\", file=outfile)\n",
    "        print(text_out, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"csv/text_diff_out.txt\", \"w\")\n",
    "for file_name in paragraphs:\n",
    "    if file_name in paragraphs_old:\n",
    "        process(file_name, outfile=outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
